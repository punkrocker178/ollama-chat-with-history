# Simple CLI Ollama chat client

## What is this
Ollama doesn't have built-in memory persistence. Which means it doesn't remember the conversation we had.  
So this script will manage and export conversations for us.  

**Note:** _Code is orginally generated by Copilot_

## How it works
We will first need to start the server using `.\ollama.exe serve` command.  
In my case, I'm using [Ollama portable on Intel GPU](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md). So I will running `start-ollama.bat` command.  

After the server has started, the script will take your prompt and call `/api/chat` endpoint. Then it will save the conversation into `chat_history.json` for storing purposes. Finally, it will also export to markdown file for our reviewing needs.

## Usage
1. Pull your favorite model
   ```
   ollama pull deepseek-r1
   ```
2. Run your model (to start the chat server)
   ```
   ollama run deepseek-r1
   ```
3. Run this script to begin chat
    ```
    python chat-with-history.py
    ```
4. If you don't like real time text popping up. You can add `--stream-disabled` argument
   ```
   python chat-with-history.py --stream-disabled
   ```
